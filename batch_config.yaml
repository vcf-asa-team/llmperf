- model: "meta/llama-3.1-8b-instruct"
  test_timeout_s: 10800
  max_num_completed_requests: 10
  mean_input_tokens: 64
  stddev_input_tokens: 8
  mean_output_tokens: 128
  stddev_output_tokens: 8
  num_concurrent_requests: 1
  results_dir: result_outputs
  llm_api: openai
- model: "meta/llama-3.1-8b-instruct"
  test_timeout_s: 10800
  max_num_completed_requests: 20
  mean_input_tokens: 64
  stddev_input_tokens: 8
  mean_output_tokens: 128
  stddev_output_tokens: 8
  num_concurrent_requests: 2
  results_dir: result_outputs
  llm_api: openai
- model: "meta/llama-3.1-8b-instruct"
  test_timeout_s: 10800
  max_num_completed_requests: 40
  mean_input_tokens: 64
  stddev_input_tokens: 8
  mean_output_tokens: 128
  stddev_output_tokens: 8
  num_concurrent_requests: 4
  results_dir: result_outputs
  llm_api: openai
- model: "meta/llama-3.1-8b-instruct"
  test_timeout_s: 10800
  max_num_completed_requests: 80
  mean_input_tokens: 64
  stddev_input_tokens: 8
  mean_output_tokens: 128
  stddev_output_tokens: 8
  num_concurrent_requests: 8
  results_dir: result_outputs
  llm_api: openai
- model: "meta/llama-3.1-8b-instruct"
  test_timeout_s: 10800
  max_num_completed_requests: 160
  mean_input_tokens: 64
  stddev_input_tokens: 8
  mean_output_tokens: 128
  stddev_output_tokens: 8
  num_concurrent_requests: 16
  results_dir: result_outputs
  llm_api: openai
- model: "meta/llama-3.1-8b-instruct"
  test_timeout_s: 10800
  max_num_completed_requests: 320
  mean_input_tokens: 64
  stddev_input_tokens: 8
  mean_output_tokens: 128
  stddev_output_tokens: 8
  num_concurrent_requests: 32
  results_dir: result_outputs
  llm_api: openai
- model: "meta/llama-3.1-8b-instruct"
  test_timeout_s: 10800
  max_num_completed_requests: 640
  mean_input_tokens: 64
  stddev_input_tokens: 8
  mean_output_tokens: 128
  stddev_output_tokens: 8
  num_concurrent_requests: 64
  results_dir: result_outputs
  llm_api: openai
- model: "meta/llama-3.1-8b-instruct"
  test_timeout_s: 10800
  max_num_completed_requests: 1280
  mean_input_tokens: 64
  stddev_input_tokens: 8
  mean_output_tokens: 128
  stddev_output_tokens: 8
  num_concurrent_requests: 128
  results_dir: result_outputs
  llm_api: openai
- model: "meta/llama-3.1-8b-instruct"
  test_timeout_s: 10800
  max_num_completed_requests: 2560
  mean_input_tokens: 64
  stddev_input_tokens: 8
  mean_output_tokens: 128
  stddev_output_tokens: 8
  num_concurrent_requests: 256
  results_dir: result_outputs
  llm_api: openai
